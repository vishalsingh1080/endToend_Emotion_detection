{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vggnet_emotion_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPmAFiTJRepMsiPVdZ34bwh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishalsingh1080/endToend_Emotion_detection/blob/main/vggnet_emotion_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy51GUZXUsIs"
      },
      "source": [
        "!pip install PIL "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FA4wcr4x-0l"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfvaF1F_QZzB"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK7wecu4Qbx_"
      },
      "source": [
        "# '/content/gdrive/My Drive/emotion_detection_vgg/fer2013.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7Afg6cWSpva"
      },
      "source": [
        "def load_data(path):\n",
        "    fer2013 = pd.read_csv(path)\n",
        "    emotion_mapping = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
        "\n",
        "    return fer2013, emotion_mapping\n",
        "\n",
        "\n",
        "def prepare_data(data):\n",
        "    \"\"\" Prepare data for modeling\n",
        "        input: data frame with labels und pixel data\n",
        "        output: image and label array \"\"\"\n",
        "\n",
        "    image_array = np.zeros(shape=(len(data), 48, 48))\n",
        "    image_label = np.array(list(map(int, data['emotion'])))\n",
        "\n",
        "    for i, row in enumerate(data.index):\n",
        "        image = np.fromstring(data.loc[row, 'pixels'], dtype=int, sep=' ')\n",
        "        image = np.reshape(image, (48, 48))\n",
        "        image_array[i] = image\n",
        "\n",
        "    return image_array, image_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipHKCyS-UgsV"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None, augment=False):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img = np.array(self.images[idx])\n",
        "\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        label = torch.tensor(self.labels[idx]).type(torch.long)\n",
        "        # tuple of PIL image & label(tensor)\n",
        "        sample = (img, label)\n",
        "\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k96IQI6RRk4"
      },
      "source": [
        "def get_dataloaders(path='datasets/fer2013/fer2013.csv', bs=64, augment=True):\n",
        "    \"\"\" Prepare train, val, & test dataloaders\n",
        "        Augment training data using:\n",
        "            - cropping\n",
        "            - shifting (vertical/horizental)\n",
        "            - horizental flipping\n",
        "            - rotation\n",
        "        input: path to fer2013 csv file\n",
        "        output: (Dataloader, Dataloader, Dataloader) \"\"\"\n",
        "\n",
        "    fer2013, emotion_mapping = load_data(path)\n",
        "\n",
        "    xtrain, ytrain = prepare_data(fer2013[fer2013['Usage'] == 'Training'])\n",
        "    xval, yval = prepare_data(fer2013[fer2013['Usage'] == 'PrivateTest'])\n",
        "    xtest, ytest = prepare_data(fer2013[fer2013['Usage'] == 'PublicTest'])\n",
        "\n",
        "    mu, st = 0, 255\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        # transforms.Scale(52),\n",
        "        transforms.TenCrop(40),\n",
        "        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
        "        transforms.Lambda(lambda tensors: torch.stack([transforms.Normalize(mean=(mu,), std=(st,))(t) for t in tensors])),\n",
        "    ])\n",
        "\n",
        "    if augment:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(48, scale=(0.8, 1.2)),\n",
        "            transforms.RandomApply([transforms.RandomAffine(0, translate=(0.2, 0.2))], p=0.5),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomApply([transforms.RandomRotation(10)], p=0.5),\n",
        "\n",
        "            transforms.TenCrop(40),\n",
        "            transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
        "            transforms.Lambda(lambda tensors: torch.stack([transforms.Normalize(mean=(mu,), std=(st,))(t) for t in tensors])),\n",
        "            transforms.Lambda(lambda tensors: torch.stack([transforms.RandomErasing(p=0.5)(t) for t in tensors])),\n",
        "        ])\n",
        "    else:\n",
        "        train_transform = test_transform\n",
        "\n",
        "    \n",
        "    train = CustomDataset(xtrain, ytrain, train_transform)\n",
        "    val = CustomDataset(xval, yval, test_transform)\n",
        "    test = CustomDataset(xtest, ytest, test_transform)\n",
        "\n",
        "    trainloader = DataLoader(train, batch_size=bs, shuffle=True, num_workers=2)\n",
        "    valloader = DataLoader(val, batch_size=64, shuffle=True, num_workers=2)\n",
        "    testloader = DataLoader(test, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "    return trainloader, valloader, testloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOaqJT8jSptV"
      },
      "source": [
        "class VggFeatures(nn.Module):\n",
        "    def __init__(self, drop=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1a = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.conv1b = nn.Conv2d(64, out_channels=64, kernel_size=3, padding=1)\n",
        "\n",
        "        self.conv2a = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv2b = nn.Conv2d(128, 128, 3, padding=1)\n",
        "\n",
        "        self.conv3a = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.conv3b = nn.Conv2d(256, 256, 3, padding=1)\n",
        "\n",
        "        self.conv4a = nn.Conv2d(256, 512, 3, padding=1)\n",
        "        self.conv4b = nn.Conv2d(512, 512, 3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.bn1a = nn.BatchNorm2d(64)\n",
        "        self.bn1b = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.bn2a = nn.BatchNorm2d(128)\n",
        "        self.bn2b = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.bn3a = nn.BatchNorm2d(256)\n",
        "        self.bn3b = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.bn4a = nn.BatchNorm2d(512)\n",
        "        self.bn4b = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.lin1 = nn.Linear(512 * 2 * 2, 4096)\n",
        "        self.lin2 = nn.Linear(4096, 4096)\n",
        "\n",
        "        self.drop = nn.Dropout(p=drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1a(self.conv1a(x)))\n",
        "        x = F.relu(self.bn1b(self.conv1b(x)))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.bn2a(self.conv2a(x)))\n",
        "        x = F.relu(self.bn2b(self.conv2b(x)))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.bn3a(self.conv3a(x)))\n",
        "        x = F.relu(self.bn3b(self.conv3b(x)))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.bn4a(self.conv4a(x)))\n",
        "        x = F.relu(self.bn4b(self.conv4b(x)))\n",
        "        x = self.pool(x)\n",
        "        # print(x.shape)\n",
        "\n",
        "        x = x.view(-1, 512 * 2 * 2)\n",
        "        x = F.relu(self.drop(self.lin1(x)))\n",
        "        x = F.relu(self.drop(self.lin2(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Vgg(VggFeatures):\n",
        "    def __init__(self, drop=0.2):\n",
        "        super().__init__(drop)\n",
        "        self.lin3 = nn.Linear(4096, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = super().forward(x)\n",
        "        x = self.lin3(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyyCQmhp6dK_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yd1RY--dy8j"
      },
      "source": [
        "# batch size = 64\n",
        "# epochs = 300\n",
        "# learning rate = 0.01\n",
        "# transformations applied to valid and test data ?"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}